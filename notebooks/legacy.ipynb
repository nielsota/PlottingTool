{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8519378b",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Copyright Â© by Boston Consulting Group. All rights reserved.\"\"\"\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# from wex.checks import pd_check_cols_in_df\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def apply_woe_transformation(\n",
    "    df: pd.DataFrame, var: str, mapper: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply weight of evidence transformation\n",
    "    :param df: Input data frame\n",
    "    :param var: Column name of variable to be transformed\n",
    "    :param mapper: Mapping table for transformation\n",
    "    :return: Original data frame with additional column\n",
    "    \"\"\"\n",
    "    var_woe = f\"{var}_woe\"\n",
    "    mapper = mapper.rename(columns={\"woe\": var_woe})\n",
    "\n",
    "    if (not pd.api.types.is_numeric_dtype(df[var])) or (\"median\" not in mapper):\n",
    "        mapper.rename(columns={\"woe\": var_woe}, inplace=True)\n",
    "        df = df.merge(mapper, how=\"left\", left_on=var, right_on=\"binned\")\n",
    "    else:\n",
    "        var_median = \"median\"\n",
    "        var_median_left = \"median_left\"\n",
    "        var_median_right = \"median_right\"\n",
    "        var_woe_left = \"woe_left\"\n",
    "        var_woe_right = \"woe_right\"\n",
    "\n",
    "        # Transform variable\n",
    "        df = df.merge(\n",
    "            mapper[\n",
    "                [\n",
    "                    \"binned\",\n",
    "                    var_median,\n",
    "                    var_median_left,\n",
    "                    var_median_right,\n",
    "                    var_woe,\n",
    "                    var_woe_left,\n",
    "                    var_woe_right,\n",
    "                ]\n",
    "            ],\n",
    "            how=\"left\",\n",
    "            on=\"binned\",\n",
    "        )\n",
    "        df.loc[\n",
    "            (df[var] < df[var_median])\n",
    "            & np.isfinite(\n",
    "                df[[var, var_median, var_median_left, var_woe, var_woe_left]]\n",
    "            ).min(axis=1),\n",
    "            var_woe,\n",
    "        ] = (\n",
    "            (df[var] - df[var_median_left]) * df[var_woe]\n",
    "            + (df[var_median] - df[var]) * df[var_woe_left]\n",
    "        ) / (\n",
    "            df[var_median] - df[var_median_left]\n",
    "        )\n",
    "        df.loc[\n",
    "            (df[var] > df[var_median])\n",
    "            & np.isfinite(\n",
    "                df[[var, var_median, var_median_right, var_woe, var_woe_right]]\n",
    "            ).min(axis=1),\n",
    "            var_woe,\n",
    "        ] = (\n",
    "            (df[var_median_right] - df[var]) * df[var_woe]\n",
    "            + (df[var] - df[var_median]) * df[var_woe_right]\n",
    "        ) / (\n",
    "            df[var_median_right] - df[var_median]\n",
    "        )\n",
    "        df.drop(\n",
    "            columns=[\n",
    "                \"binned\",\n",
    "                var_median,\n",
    "                var_median_left,\n",
    "                var_median_right,\n",
    "                var_woe_left,\n",
    "                var_woe_right,\n",
    "            ],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_woe_mapping_table(\n",
    "    df: pd.DataFrame, n_bins: int, var_median: str, var_woe: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create mapping table for WOE transformation based on binning results\n",
    "    :param n_bins: Number of bins\n",
    "    :param df: Binning results\n",
    "    :param var_median: Column name in res for the median of var\n",
    "    :param var_woe: Column name in res for the WOE of var\n",
    "    :return: Mapping table\n",
    "    \"\"\"\n",
    "    mapper = (\n",
    "        df[[\"binned\", var_median, var_woe]]\n",
    "        .rename(columns={var_median: \"median\", var_woe: \"woe\"})\n",
    "        .sort_values(\"binned\")\n",
    "    )\n",
    "\n",
    "    mapper[\"woe_left\"] = mapper[\"woe\"].shift(1)\n",
    "    mapper[\"woe_right\"] = mapper[\"woe\"].shift(-1)\n",
    "    mapper[\"median_left\"] = mapper[\"median\"].shift(1)\n",
    "    mapper[\"median_right\"] = mapper[\"median\"].shift(-1)\n",
    "\n",
    "    # Add replacement values for empty bins\n",
    "    for b in list(range(0, n_bins)) + list(range(n_bins - 1, -1, -1)):\n",
    "        if sum(mapper.binned == b) == 0:\n",
    "            left = np.nan\n",
    "            right = np.nan\n",
    "            if (len(mapper.loc[mapper.binned == b - 1]) == 1) & (b > 0):\n",
    "                left = map.loc[map.binned == b - 1, \"woe\"].values[0]\n",
    "            if (len(mapper.loc[mapper.binned == b + 1]) == 1) & (b < n_bins - 1):\n",
    "                right = map.loc[map.binned == b + 1, \"woe\"].values[0]\n",
    "            val = np.nanmean([left, right])\n",
    "            if not np.isnan(val):\n",
    "                tmp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"binned\": [b],\n",
    "                        \"median\": [np.nan],\n",
    "                        \"median_left\": [np.nan],\n",
    "                        \"median_right\": [np.nan],\n",
    "                        \"woe\": [val],\n",
    "                        \"woe_left\": [np.nan],\n",
    "                        \"woe_right\": [np.nan],\n",
    "                    }\n",
    "                )\n",
    "                mapper = mapper.append(tmp)\n",
    "\n",
    "    # Check that the table include replacements for nan and inf\n",
    "    if sum(mapper.binned == -1) == 0:\n",
    "        val = mapper.loc[mapper.binned == 0, \"woe\"].values[0]\n",
    "        tmp = pd.DataFrame(\n",
    "            {\n",
    "                \"binned\": [-1],\n",
    "                \"median\": [-np.inf],\n",
    "                \"median_left\": [np.nan],\n",
    "                \"median_right\": [np.nan],\n",
    "                \"woe\": [val],\n",
    "                \"woe_left\": [np.nan],\n",
    "                \"woe_right\": [np.nan],\n",
    "            }\n",
    "        )\n",
    "        mapper = mapper.append(tmp)\n",
    "    if sum(mapper.binned == n_bins) == 0:\n",
    "        val = mapper.loc[mapper.binned == (n_bins - 1), \"woe\"].values[0]\n",
    "        tmp = pd.DataFrame(\n",
    "            {\n",
    "                \"binned\": [n_bins],\n",
    "                \"median\": [np.inf],\n",
    "                \"median_left\": [np.nan],\n",
    "                \"median_right\": [np.nan],\n",
    "                \"woe\": [val],\n",
    "                \"woe_left\": [np.nan],\n",
    "                \"woe_right\": [np.nan],\n",
    "            }\n",
    "        )\n",
    "        mapper = mapper.append(tmp)\n",
    "    if sum(mapper.binned == n_bins + 1) == 0:\n",
    "        tmp = pd.DataFrame(\n",
    "            {\n",
    "                \"binned\": [n_bins + 1],\n",
    "                \"median\": [np.nan],\n",
    "                \"median_left\": [np.nan],\n",
    "                \"median_right\": [np.nan],\n",
    "                \"woe\": [0.0],\n",
    "                \"woe_left\": [np.nan],\n",
    "                \"woe_right\": [np.nan],\n",
    "            }\n",
    "        )\n",
    "        mapper = mapper.append(tmp)\n",
    "\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def create_woe_transformation(\n",
    "    df: pd.DataFrame,\n",
    "    target: str,\n",
    "    var: str,\n",
    "    n_bins: int = 10,\n",
    "    btype: str = \"number\",\n",
    "    bins: np.ndarray = None,\n",
    "    min_var: Any = None,\n",
    "    max_var: Any = None,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create weight of evidence plots\n",
    "    :param df: Input data frame\n",
    "    :param target: Column name of target variable\n",
    "    :param var: Column name to calculate WOE/IV for\n",
    "    :param n_bins: Number of bins\n",
    "    :param btype: Method for creating bins\n",
    "    :param bins: Predefined bins\n",
    "    :param min_var: Lower bound of bin thresholds\n",
    "    :param max_var: Upper bound of bin thresholds\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _logger.info(f\"Analyzing variable {var}...\")\n",
    "\n",
    "    df = df[[target, var]].copy()\n",
    "    n_bad = df[target].sum()\n",
    "    n_obs = df[target].count()\n",
    "    n_miss = df[var].isna().sum()\n",
    "\n",
    "    if bins is not None:\n",
    "        bins = pd.unique(bins)\n",
    "        n_bins = len(bins) + 1\n",
    "\n",
    "    if df[var].count() == 0:\n",
    "        # All rows missing\n",
    "        power = pd.DataFrame(\n",
    "            {\"name\": [var], \"auc\": [0.5], \"auc_woe\": [0.5], \"miss_pc\": [100], \"iv\": [0]}\n",
    "        )\n",
    "        _logger.info(f\"Variable {var} has no valid values.\")\n",
    "        return None, None, power, None\n",
    "\n",
    "    if (not pd.api.types.is_numeric_dtype(df[var])) or (df[var].nunique() < n_bins):\n",
    "        # Calculate results for categorical variables\n",
    "        res = (\n",
    "            df.groupby(var, dropna=False)\n",
    "            .agg({target: [\"count\", \"mean\", \"sum\"]})\n",
    "            .reset_index()\n",
    "        )\n",
    "        level_one = res.columns.get_level_values(0).astype(str)\n",
    "        level_two = res.columns.get_level_values(1).astype(str)\n",
    "        column_separator = [\"_\" if x != \"\" else \"\" for x in level_two]\n",
    "        res.columns = level_one + column_separator + level_two\n",
    "\n",
    "        var_woem = f\"{var}_woem\"\n",
    "        res[f\"{target}_mean\"] = np.clip(res[f\"{target}_mean\"], 1e-4, 1 - 1e-4)\n",
    "        res[var_woem] = np.log((1.0 / res[f\"{target}_mean\"]) - 1.0) + np.log(\n",
    "            n_bad / (n_obs - n_bad)\n",
    "        )\n",
    "        res[\"iv\"] = (\n",
    "            (res[f\"{target}_count\"] - res[f\"{target}_sum\"]) / (n_obs - n_bad)\n",
    "            - (res[f\"{target}_sum\"] / n_bad)\n",
    "        ) * res[var_woem]\n",
    "        iv = res[\"iv\"].sum()\n",
    "        res.rename(columns={var: \"binned\"}, inplace=True)\n",
    "\n",
    "        mapper = res.copy()\n",
    "        mapper.rename(columns={var_woem: \"woe\"}, inplace=True)\n",
    "        mapper.drop(\n",
    "            columns=[f\"{target}_count\", f\"{target}_mean\", f\"{target}_sum\", \"iv\"],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        if len(mapper.loc[mapper.binned.isnull()]) == 0:\n",
    "            mapper = mapper.append(pd.DataFrame({\"binned\": [np.nan], \"woe\": [0.0]}))\n",
    "    else:\n",
    "        # Calculate results for continuous variables\n",
    "        if bins is None:\n",
    "            if btype == \"distance\":\n",
    "                # Replace outliers\n",
    "                if min_var is None:\n",
    "                    min_var = df.loc[\n",
    "                        ~df[var].isin([-np.inf, np.inf, np.nan]), var\n",
    "                    ].quantile(0.1)\n",
    "                if max_var is None:\n",
    "                    max_var = df.loc[\n",
    "                        ~df[var].isin([-np.inf, np.inf, np.nan]), var\n",
    "                    ].quantile(0.9)\n",
    "                bins = np.linspace(min_var, max_var, n_bins - 1)\n",
    "                bins = pd.unique(bins)\n",
    "                n_bins = len(bins) + 1\n",
    "            else:\n",
    "                percentile = np.linspace(1.0 / n_bins, 1.0 - 1.0 / n_bins, n_bins - 1)\n",
    "                bins = df.loc[~df[var].isin([-np.inf, np.inf, np.nan]), var].quantile(\n",
    "                    percentile\n",
    "                )\n",
    "                bins = bins.unique()\n",
    "                n_bins = len(bins) + 1\n",
    "\n",
    "        df[\"binned\"] = np.digitize(df[[var]], bins)\n",
    "        df[\"binned\"] = np.where(df[var] == -np.inf, -1, df[\"binned\"])\n",
    "        df[\"binned\"] = np.where(df[var] == np.inf, n_bins, df[\"binned\"])\n",
    "        df[\"binned\"] = np.where(df[var].isna(), n_bins + 1, df[\"binned\"])\n",
    "        res = (\n",
    "            df.groupby(\"binned\")\n",
    "            .agg({var: [\"mean\", \"median\"], target: [\"count\", \"mean\", \"sum\"]})\n",
    "            .reset_index()\n",
    "        )\n",
    "        level_one = res.columns.get_level_values(0).astype(str)\n",
    "        level_two = res.columns.get_level_values(1).astype(str)\n",
    "        column_separator = [\"_\" if x != \"\" else \"\" for x in level_two]\n",
    "        res.columns = level_one + column_separator + level_two\n",
    "\n",
    "        var_median = f\"{var}_median\"\n",
    "        var_woem = f\"{var}_woem\"\n",
    "        res[f\"{target}_mean\"] = np.clip(res[f\"{target}_mean\"], 1e-4, 1 - 1e-4)\n",
    "        res[var_woem] = np.log((1.0 / res[f\"{target}_mean\"]) - 1.0) + np.log(\n",
    "            n_bad / (n_obs - n_bad)\n",
    "        )\n",
    "        res[\"iv\"] = (\n",
    "            (res[f\"{target}_count\"] - res[f\"{target}_sum\"]) / (n_obs - n_bad)\n",
    "            - (res[f\"{target}_sum\"] / n_bad)\n",
    "        ) * res[var_woem]\n",
    "        iv = res[\"iv\"].sum()\n",
    "\n",
    "        # Create mapping table for woe transformation\n",
    "        mapper = create_woe_mapping_table(res, n_bins, var_median, var_woem)\n",
    "\n",
    "    # Apply woe transformation\n",
    "    df = apply_woe_transformation(df, var, mapper)\n",
    "    df.sort_values(by=var, inplace=True)\n",
    "\n",
    "    # Calculate AUCs for WOE transformed and original variable\n",
    "    fpr_woe, tpr_woe, _ = metrics.roc_curve(df[target], -df[f\"{var}_woe\"], pos_label=1)\n",
    "    auc_woe = metrics.auc(fpr_woe, tpr_woe)\n",
    "    if pd.api.types.is_numeric_dtype(df[var]):\n",
    "        tmp = df.loc[~df[var].isin([-np.inf, np.inf, np.nan])]\n",
    "        fpr, tpr, _ = metrics.roc_curve(tmp[target], -tmp[var], pos_label=1)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    # Plot default rate and number of observations for each bin\n",
    "    _, (al, ar) = plt.subplots(\n",
    "        ncols=2, figsize=(16, 4), gridspec_kw={\"width_ratios\": [1, 1]}\n",
    "    )\n",
    "\n",
    "    if (pd.api.types.is_numeric_dtype(df[var])) and (df[var].nunique() >= len(mapper)):\n",
    "        # Group in many bins to plot woe, only for numeric variables\n",
    "        n_plot_bins = 25\n",
    "        percentiles = np.linspace(1.0 / n_plot_bins, 1.0, n_plot_bins)\n",
    "        plot_bins = df.loc[~df[var].isin([-np.inf, np.inf, np.nan]), var].quantile(\n",
    "            percentiles\n",
    "        )\n",
    "        plot_bins.iloc[n_plot_bins - 1] = plot_bins.iloc[n_plot_bins - 1] * (1 + 1e-6)\n",
    "\n",
    "        df[\"plotbin\"] = np.digitize(df[[var]], plot_bins)\n",
    "        df[\"plotbin\"] = np.where(df[var] == -np.inf, -1, df[\"plotbin\"])\n",
    "        df[\"plotbin\"] = np.where(df[var] == np.inf, n_plot_bins, df[\"plotbin\"])\n",
    "        df[\"plotbin\"] = np.where(df[var].isna(), n_plot_bins + 1, df[\"plotbin\"])\n",
    "        plotres = (\n",
    "            df.groupby(\"plotbin\")\n",
    "            .agg({var: [\"median\"], f\"{var}_woe\": [\"median\"]})\n",
    "            .reset_index()\n",
    "        )\n",
    "        level_one = plotres.columns.get_level_values(0).astype(str)\n",
    "        level_two = plotres.columns.get_level_values(1).astype(str)\n",
    "        column_separator = [\"_\" if x != \"\" else \"\" for x in level_two]\n",
    "        plotres.columns = level_one + column_separator + level_two\n",
    "\n",
    "        xx = res[\"binned\"]\n",
    "        xx_r = plotres[\"plotbin\"]\n",
    "        yy = plotres[f\"{var}_woe_median\"]\n",
    "        labels = res[var_median].map(\"{:,.2f}\".format)\n",
    "        labels_r = plotres[var_median].map(\"{:,.2f}\".format)\n",
    "    else:\n",
    "        yy = res[var_woem]\n",
    "        xx = np.arange(res.index.size)\n",
    "        xx_r = xx\n",
    "        labels = res[\"binned\"].astype(str).str[:12]\n",
    "        labels_r = labels\n",
    "\n",
    "    al.set_xlabel(var)\n",
    "    al.set_ylabel(\"Clients\")\n",
    "    al.set_xticks(xx)\n",
    "    al.set_xticklabels(labels, rotation=\"vertical\")\n",
    "    al.bar(xx, res[f\"{target}_count\"])\n",
    "    al2 = al.twinx()\n",
    "    al2.set_ylabel(\"Bad rate\")\n",
    "    plt.yscale(\"log\")\n",
    "    al2.plot(\n",
    "        np.arange(res.index.size),\n",
    "        res[f\"{target}_mean\"],\n",
    "        color=\"darkorange\",\n",
    "        label=\"bad_rate\",\n",
    "    )\n",
    "    al2.legend(loc=0)\n",
    "    plt.subplots_adjust(bottom=0.2, wspace=0.3)\n",
    "\n",
    "    ar.set_xlabel(var)\n",
    "    ar.set_ylabel(\"Weight of evidence\")\n",
    "    ar.set_xticks(xx_r)\n",
    "    ar.set_xticklabels(labels_r, rotation=\"vertical\")\n",
    "    ar.plot(xx_r, yy, color=\"darkorange\", label=\"woe\")\n",
    "    ar.legend(loc=0)\n",
    "    plt.subplots_adjust(bottom=0.2, wspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC curves\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(\n",
    "        fpr_woe,\n",
    "        tpr_woe,\n",
    "        color=\"darkorange\",\n",
    "        lw=2,\n",
    "        label=f\"{var}_woe (area = {auc_woe:.3f})\",\n",
    "    )\n",
    "    if pd.api.types.is_numeric_dtype(df[var]):\n",
    "        ax.plot(fpr, tpr, color=\"navy\", lw=2, label=f\"{var} (area = {auc:.3f})\")\n",
    "    ax.plot([0, 1], [0, 1], color=\"lightgrey\", lw=1, linestyle=\"--\")\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"Cumulated goods\")\n",
    "    ax.set_ylabel(\"Cumulated bads\")\n",
    "    ax.set_title(\"ROC curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    _logger.info(f\"Information value: {iv}\")\n",
    "    _logger.info(f\"AUC (WOE): {auc_woe}\")\n",
    "    _logger.info(f\"AUC: {auc}\")\n",
    "    if pd.api.types.is_numeric_dtype(df[var]):\n",
    "        quantiles = df[var].quantile([0, 0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99, 1])\n",
    "        _logger.info(f\"Quantiles:\\n{quantiles}\")\n",
    "\n",
    "    power = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": [var],\n",
    "            \"auc\": [auc],\n",
    "            \"auc_woe\": [auc_woe],\n",
    "            \"miss_pc\": [100 * n_miss / n_obs],\n",
    "            \"iv\": [iv],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return bins, mapper, power, res\n",
    "\n",
    "\n",
    "def create_binary_target_plots(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    feature_col: str,\n",
    "    n_bins: int = 10,\n",
    "    min_feature_val: float = None,\n",
    "    max_feature_val: float = None,\n",
    "    min_feature_q: float = None,\n",
    "    max_feature_q: float = None,\n",
    "    create_plot=True,\n",
    "    invert_xaxis=False,\n",
    "    use_quantiles: bool = True,\n",
    "    fillna: bool = False\n",
    ") -> Tuple[Union[pd.DataFrame, None], Union[plt.figure, None]]:\n",
    "    \"\"\"Creates three plots for univariate analysis for binary targets.\n",
    "    :param df: Input data frame\n",
    "    :param target_col: Column name of target variable\n",
    "    :param feature_col: List of column names contained in df.\n",
    "    :param n_bins: Number of bins\n",
    "    :param min_feature_val: Lower feature value at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param max_feature_val Upper feature value at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param min_feature_q: Lower quantile at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param max_feature_q: Upper quantile at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param create_plot: Determines whether figure will be plotted.\n",
    "    :param use_quantiles: Whether to use quantiles for event rate bins.\n",
    "    :return: -> Dataframe containing feature and target metrics and figure:\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure columns exist\n",
    "    # pd_check_cols_in_df(df, col_list=[feature_col, target_col], raise_flag=True)\n",
    "\n",
    "    if df[feature_col].nunique() < 2:\n",
    "        _logger.info(\"Feature contains less than 2 features!\")\n",
    "        return None, None\n",
    "\n",
    "    # Ensure target contains no NA values\n",
    "    if df[target_col].isna().sum() > 0:\n",
    "        raise ValueError(\"Target column contains NA values!\")\n",
    "\n",
    "    # Ensure target is binary\n",
    "    if df[target_col].nunique() != 2:\n",
    "        raise ValueError(\"Target variable is not binary!\")\n",
    "\n",
    "    # Ensure min/max values are in correct order\n",
    "    if (\n",
    "        min_feature_val is not None\n",
    "        and max_feature_val is not None\n",
    "        and min_feature_val >= max_feature_val\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"min_feature_value must be strictly smaller than max_feature_val!\"\n",
    "        )\n",
    "    elif (\n",
    "        min_feature_q is not None\n",
    "        and max_feature_q is not None\n",
    "        and min_feature_q >= max_feature_q\n",
    "    ):\n",
    "        raise ValueError(\"min_feature_q must be strictly smaller than max_feature_q!\")\n",
    "    elif min_feature_q is not None and min_feature_q < 0:\n",
    "        raise ValueError(\"Quantiles must be values between 0 and 1!!\")\n",
    "    elif max_feature_q is not None and max_feature_q < 0:\n",
    "        raise ValueError(\"Quantiles must be values between 0 and 1!!\")\n",
    "\n",
    "    # Create fresh copy of dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    #if fillna == True:\n",
    "    #        df = df.copy()\n",
    "    #        df.loc[df[feature_col].isin([-np.inf, np.inf, np.nan]), feature_col] = df.loc[~df[feature_col].isin([-np.inf, np.inf, np.nan]), feature_col].median()\n",
    "\n",
    "    # Calculate global event rate\n",
    "    event_rate = np.mean(df[target_col])\n",
    "\n",
    "    # Case 1: At most n_bins elements\n",
    "    if df[feature_col].nunique() < n_bins:\n",
    "\n",
    "        # Extract unique values\n",
    "        unique_vals = df[feature_col].sort_values().unique()\n",
    "\n",
    "        # Convert sorted elements to int\n",
    "        mapping = pd.factorize(unique_vals, na_sentinel=len(unique_vals))\n",
    "\n",
    "        # Create mapping\n",
    "        mapping_values = list(mapping[0])\n",
    "        mapping_keys = list(mapping[1])\n",
    "\n",
    "        # Create mapping dictionary\n",
    "        mapping_dict = dict(zip(mapping_keys, mapping_values))\n",
    "\n",
    "        # Apply mapping to column\n",
    "        df = df.assign(bins=df[feature_col].map(mapping_dict))\n",
    "\n",
    "        # Set labels for plotting\n",
    "        labels = mapping_keys\n",
    "\n",
    "    # Case 2: More than n_bins elements and categorical\n",
    "    elif (not pd.api.types.is_numeric_dtype(df[feature_col])) and (\n",
    "        df[feature_col].nunique() >= n_bins\n",
    "    ):\n",
    "        _logger.info(\n",
    "            f\"--- Number of unique elements ({df[feature_col].nunique()} \"\n",
    "            f\"exceeds n_bins ({n_bins})). \"\n",
    "            f\"Increase n_bins to create a plot!\"\n",
    "        )\n",
    "        return None, None\n",
    "\n",
    "    # Case 3: More than n_bins elements and numerical\n",
    "    elif pd.api.types.is_numeric_dtype(df[feature_col]):\n",
    "\n",
    "        # Extract number of unique values\n",
    "        n_unique_feat_vals = df[feature_col].nunique()\n",
    "\n",
    "        # Adjust n_bins is less unique values exist\n",
    "        n_bins = np.minimum(n_unique_feat_vals, n_bins)\n",
    "\n",
    "        min_val = np.nanmin(df[feature_col])\n",
    "        max_val = np.nanmax(df[feature_col])\n",
    "\n",
    "        if min_feature_q is not None:\n",
    "            min_val = np.nanquantile(df[feature_col], min_feature_q)\n",
    "\n",
    "        if max_feature_q is not None:\n",
    "            max_val = np.nanquantile(df[feature_col], max_feature_q)\n",
    "\n",
    "        if min_feature_val is not None:\n",
    "            min_val = np.maximum(min_val, min_feature_val)\n",
    "\n",
    "        if max_feature_val is not None:\n",
    "            max_val = np.minimum(max_val, max_feature_val)\n",
    "\n",
    "        idx_neg_inf = df[feature_col] == -np.inf\n",
    "        idx_pos_inf = df[feature_col] == np.inf\n",
    "        min_val_excl_inf = np.nanmin(df[feature_col][~idx_neg_inf])\n",
    "        max_val_excl_inf = np.nanmax(df[feature_col][~idx_pos_inf])\n",
    "\n",
    "        min_val_adj = np.maximum(min_val, min_val_excl_inf)\n",
    "        max_val_adj = np.minimum(max_val, max_val_excl_inf)\n",
    "\n",
    "        # Create equidistant grid\n",
    "        if use_quantiles:\n",
    "            bins = np.unique(\n",
    "                df[feature_col]\n",
    "                .clip(min_val_adj, max_val_adj)\n",
    "                .quantile(np.linspace(0, 1, n_bins))\n",
    "            )\n",
    "            n_bins = len(bins)\n",
    "        else:\n",
    "            bins = np.linspace(min_val_adj, max_val_adj, n_bins)\n",
    "\n",
    "        bins[0] = min_val\n",
    "        bins[n_bins - 1] = max_val\n",
    "\n",
    "        # Clip values according to min/max values\n",
    "        df[feature_col].clip(lower=min_val, upper=max_val, inplace=True)\n",
    "\n",
    "        # Ensure clipping values dooes not remove all but a single value\n",
    "        if df[feature_col].nunique() < 2:\n",
    "            _logger.info(\n",
    "                \"Feature contains less than 2 features after clipping outliers!!\"\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "        # Create bins (return None if binning is not successfull)\n",
    "        try:\n",
    "            df = df.assign(\n",
    "                bins=pd.cut(\n",
    "                    x=df.loc[:, feature_col],\n",
    "                    bins=bins,\n",
    "                    include_lowest=True,\n",
    "                    right=True,\n",
    "                    labels=False,\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            _logger.warning(e)\n",
    "            return None, None\n",
    "\n",
    "        # Create plot labels\n",
    "        bins = list(bins)\n",
    "        labels = [\n",
    "            f'({\"{:,.2f}\".format(bins[i])}, {\"{:,.2f}\".format(bins[i+1])}]'\n",
    "            for i in range(n_bins - 1)\n",
    "        ]\n",
    "\n",
    "        # Handle NAs\n",
    "        if df[\"bins\"].isna().sum() > 0:\n",
    "            df.loc[:, \"bins\"] = df.loc[:, \"bins\"].where(\n",
    "                ~df.loc[:, \"bins\"].isna(), n_bins - 1\n",
    "            )\n",
    "            labels.append(\"NA\")\n",
    "            n_bins += 1\n",
    "\n",
    "        # Convert bins to categories\n",
    "        df.bins = df.bins.astype(\"category\")\n",
    "\n",
    "        # Set all categories\n",
    "        df.bins = df.bins.cat.set_categories(list(range(n_bins - 1)))\n",
    "\n",
    "    # Group into bins and calculate required metrics\n",
    "    df_binned = df.groupby(\"bins\").agg({feature_col: [len], target_col: [\"mean\"]})\n",
    "\n",
    "    # Rename columns\n",
    "    level_one = df_binned.columns.get_level_values(0).astype(str)\n",
    "    level_two = df_binned.columns.get_level_values(1).astype(str)\n",
    "    column_separator = [\"_\" if x != \"\" else \"\" for x in level_two]\n",
    "    df_binned.columns = level_one + column_separator + level_two\n",
    "\n",
    "    # Set NA counts to zero\n",
    "    df_binned[f\"{feature_col}_len\"] = df_binned[f\"{feature_col}_len\"].fillna(0)\n",
    "\n",
    "    # Add lift rate\n",
    "    df_binned.loc[:, \"lift_rate\"] = np.divide(\n",
    "        df_binned[f\"{target_col}_mean\"], event_rate\n",
    "    )\n",
    "\n",
    "    # Build plots dependent on feature type\n",
    "    if not pd.api.types.is_numeric_dtype(df[feature_col]):\n",
    "\n",
    "        grid = plt.GridSpec(1, 6, wspace=1.2, hspace=0.2)\n",
    "        fig_grid = plt.figure(figsize=(18, 12))\n",
    "        fig_grid.suptitle(f\"{feature_col}: Event Rates\", fontsize=20)\n",
    "\n",
    "    else:\n",
    "\n",
    "        grid = plt.GridSpec(2, 6, wspace=1.2, hspace=0.2, height_ratios=[2, 2])\n",
    "        fig_grid = plt.figure(figsize=(14, 9))\n",
    "        fig_grid.suptitle(\n",
    "            f\"{feature_col}: Roc Curve | Densities | Event Rates\", fontsize=20\n",
    "        )\n",
    "        \n",
    "        if fillna == True:\n",
    "            df_imputed = df.copy()\n",
    "            df_imputed.loc[df_imputed[feature_col].isin([-np.inf, np.inf, np.nan]), feature_col] = df_imputed.loc[~df_imputed[feature_col].isin([-np.inf, np.inf, np.nan]), feature_col].median()\n",
    "            \n",
    "        else:\n",
    "            df_imputed = df.loc[~df[feature_col].isin([-np.inf, np.inf, np.nan])]\n",
    "        fpr, tpr, _ = metrics.roc_curve(\n",
    "            df_imputed[target_col], -df_imputed[feature_col], pos_label=1\n",
    "        )\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        if auc < 0.5:\n",
    "            fpr, tpr, _ = metrics.roc_curve(\n",
    "                df_imputed[target_col], df_imputed[feature_col], pos_label=1\n",
    "            )\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        # Build Roc-Curve plot\n",
    "        upper_ax_left = fig_grid.add_subplot(grid[0, 0:3])\n",
    "        if pd.api.types.is_numeric_dtype(df[feature_col]):\n",
    "            upper_ax_left.plot(\n",
    "                fpr, tpr, lw=2, label=f\"{feature_col} (area = {auc:.3f})\"\n",
    "            )\n",
    "\n",
    "        upper_ax_left.plot([0, 1], [0, 1], color=\"lightgrey\", lw=1, linestyle=\"--\")\n",
    "        upper_ax_left.set_xlim([0.0, 1.0])\n",
    "        upper_ax_left.set_ylim([0.0, 1.05])\n",
    "        upper_ax_left.set_xlabel(\"Cumulated goods\")\n",
    "        upper_ax_left.set_ylabel(\"Cumulated bads\")\n",
    "        upper_ax_left.legend(loc=\"lower right\")\n",
    "\n",
    "        # Build density plot\n",
    "        upper_ax_right = fig_grid.add_subplot(grid[0, 3:6])\n",
    "\n",
    "        sns.kdeplot(\n",
    "            data=df,\n",
    "            x=feature_col,\n",
    "            hue=target_col,\n",
    "            log_scale=False,\n",
    "            fill=True,\n",
    "            cumulative=False,\n",
    "            common_norm=False,\n",
    "            ax=upper_ax_right,\n",
    "            clip=(df[feature_col].quantile(0.01), df[feature_col].quantile(0.99))\n",
    "        )\n",
    "        if all(df[feature_col] > 0):\n",
    "            upper_ax_right.set_xscale(\"log\")\n",
    "\n",
    "    # Build event rate plot\n",
    "    if pd.api.types.is_numeric_dtype(df[feature_col]):\n",
    "        lower_ax = fig_grid.add_subplot(grid[1, :])\n",
    "    else:\n",
    "        lower_ax = fig_grid.add_subplot(grid[0, :])\n",
    "\n",
    "    # Add ticks & laels to axis\n",
    "    lower_ax.set_xlabel(feature_col)\n",
    "    lower_ax.set_ylabel(\"# Observations\")\n",
    "    lower_ax.set_xticks(df_binned.index)\n",
    "    lower_ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "    # Plot barplot containing number of observations\n",
    "    lower_ax.bar(df_binned.index, df_binned[f\"{feature_col}_len\"], color=\"lightgray\")\n",
    "\n",
    "    lower_ax.yaxis.grid(False)\n",
    "\n",
    "    # Mirror plot and add event rates\n",
    "    lower_ax_2 = lower_ax.twinx()\n",
    "    lower_ax_2.set_ylabel(\"Event rate\")\n",
    "    lower_ax_2.set_ylim(ymin=0, ymax=df_binned[f\"{target_col}_mean\"].max() + 0.1)\n",
    "    plt.yticks(np.arange(0, df_binned[f\"{target_col}_mean\"].max() + 0.1, step=0.1))\n",
    "    lower_ax_2.plot(\n",
    "        df_binned.index, df_binned[f\"{target_col}_mean\"], label=\"event_rate\", marker=\"o\"\n",
    "    )\n",
    "\n",
    "    # Add global event rate as baseline\n",
    "    lower_ax_2.plot(\n",
    "        [min(df_binned.index) - 1, max(df_binned.index) + 1],\n",
    "        [event_rate, event_rate],\n",
    "        color=\"darkgrey\",\n",
    "        lw=1,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"total_event_rate\\n({'{:.1%}'.format(event_rate)})\",\n",
    "    )\n",
    "    lower_ax_2.legend(loc=0)\n",
    "    lower_ax_2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=0))\n",
    "\n",
    "    lower_ax_2.yaxis.grid(False)\n",
    "    lower_ax_2.set_xlim([min(df_binned.index) - 0.5, max(df_binned.index) + 0.5])\n",
    "\n",
    "    if invert_xaxis:\n",
    "        lower_ax_2.invert_xaxis()\n",
    "\n",
    "    if create_plot:\n",
    "        plt.show()\n",
    "\n",
    "    return df_binned, fig_grid\n",
    "\n",
    "\n",
    "def create_save_binary_target_plots(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    feature_col_list: Union[None, List[str]],\n",
    "    path: Union[Path, str],\n",
    "    n_bins: int = 10,\n",
    "    min_feature_q: Union[None, float] = None,\n",
    "    max_feature_q: Union[None, float] = None,\n",
    "    use_quantiles: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"A wrapper to create binary target plots and save to disk for multiple features.\n",
    "    :param df: Input data frame\n",
    "    :param target_col: Column name of target variable\n",
    "    :param feature_col_list: List of feature names to consider.\n",
    "    :param path: The path where plots should be saved.\n",
    "    :param n_bins: Number of bins\n",
    "    :param min_feature_q: Lower quantile at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param max_feature_q: Upper quantile at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param use_quantiles: Whether to use quantiles for event rate bins.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Use all columns if no feature list is provided\n",
    "    if feature_col_list is None:\n",
    "        feature_col_list = [col for col in df.columns if col != target_col]\n",
    "\n",
    "    # Ensure all columns are present in data\n",
    "    # pd_check_cols_in_df(df=df, col_list=feature_col_list, raise_flag=True)\n",
    "\n",
    "    # Loop through feature list and save figure\n",
    "    for i in tqdm(range(len(feature_col_list))):\n",
    "\n",
    "        # Extract feature name\n",
    "        feature_name = feature_col_list[i]\n",
    "\n",
    "        # Create filename\n",
    "        fpath = path.joinpath(f\"{feature_name}.png\")\n",
    "\n",
    "        # Create output\n",
    "        df_binned, fig_grid = create_binary_target_plots(\n",
    "            df=df,\n",
    "            target_col=target_col,\n",
    "            feature_col=feature_name,\n",
    "            n_bins=n_bins,\n",
    "            min_feature_val=None,\n",
    "            max_feature_val=None,\n",
    "            min_feature_q=min_feature_q,\n",
    "            max_feature_q=max_feature_q,\n",
    "            create_plot=False,\n",
    "            use_quantiles=use_quantiles,\n",
    "        )\n",
    "\n",
    "        if fig_grid is not None:\n",
    "            fig_grid.savefig(fname=fpath, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def create_univariate_feature_target_stats(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    feature_col_list: Union[None, List[str]],\n",
    "    path: Union[Path, str],\n",
    "    folder_suffix=None,\n",
    "    n_bins: int = 10,\n",
    "    n_elem_max: int = 20,\n",
    "    min_feature_q: Union[None, float] = None,\n",
    "    max_feature_q: Union[None, float] = None,\n",
    "    use_quantiles: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Saves binary feature target plots and metrics for a given sample.\n",
    "    :param df: Input data frame\n",
    "    :param target_col: Column name of target variable\n",
    "    :param feature_col_list: List of feature names to consider.\n",
    "    :param path: The path where plots should be saved.\n",
    "    :param n_bins: Number of bins\n",
    "    :param n_elem_max: Maximum number of elements for non-numeric columns.\n",
    "    :param min_feature_q: Lower quantile at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param max_feature_q: Upper quantile at which to\n",
    "        winsorize (only applied to numerical columns).\n",
    "    :param use_quantiles: Whether to use quantiles for event rate bins.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if feature_col_list is None:\n",
    "        feature_col_list = df.columns.tolist()\n",
    "    else:\n",
    "        feature_col_list = feature_col_list + [target_col]\n",
    "\n",
    "    # Ensure columns are present\n",
    "    # _ = pd_check_cols_in_df(df=df, col_list=feature_col_list, raise_flag=True)\n",
    "\n",
    "    # Select column subset\n",
    "    df = df[feature_col_list]\n",
    "\n",
    "    # Create folder name\n",
    "    folder_name = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    if folder_suffix is not None:\n",
    "        folder_name = f\"{folder_name}_{folder_suffix}\"\n",
    "\n",
    "    # Create output path\n",
    "    output_path = Path(path).joinpath(folder_name)\n",
    "\n",
    "    # Creat folder\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    _logger.info(f\"--- Saving output to: {output_path} ---\")\n",
    "\n",
    "    # Create subfolder\n",
    "    output_path_plot = output_path.joinpath(\"plots\")\n",
    "    output_path_plot.mkdir(parents=True, exist_ok=True)\n",
    "    output_path_metrics = output_path.joinpath(\"metrics\")\n",
    "    output_path_metrics.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build and save plots\n",
    "    _logger.info(\"--- Create binary target plots ---\")\n",
    "    create_save_binary_target_plots(\n",
    "        df=df,\n",
    "        target_col=target_col,\n",
    "        feature_col_list=None,\n",
    "        n_bins=n_bins,\n",
    "        min_feature_q=min_feature_q,\n",
    "        max_feature_q=max_feature_q,\n",
    "        path=output_path_plot,\n",
    "        use_quantiles=use_quantiles,\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_univariate_aucs(\n",
    "    sample: pd.DataFrame, \n",
    "    targets: List, \n",
    "    features_per_module: Dict, \n",
    "    feature_to_flag: Dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Saves binary feature target plots and metrics for a given sample.\n",
    "    :param sample: Input data frame\n",
    "    :param targets: List of target column names\n",
    "    :param features_per_module: Dictionary of feature by module.\n",
    "    :param feature_to_flag: Dictionary of module flags.\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    stat_df = pd.DataFrame()\n",
    "\n",
    "    all_features = features_per_module['account'] + features_per_module['transaction'] + features_per_module['delinquency']\n",
    "    \n",
    "\n",
    "    for f in all_features:\n",
    "        \n",
    "        flag = feature_to_flag[f]\n",
    "        \n",
    "        \n",
    "        flagged_df = sample.copy() #sample[sample[flag].eq(1.0)].copy()\n",
    "\n",
    "        missing = flagged_df[f].isin([-np.inf, np.inf, np.nan]).mean()\n",
    "        sub_df_dropna = flagged_df.loc[~flagged_df[f].isin([-np.inf, np.inf, np.nan])]\n",
    "\n",
    "        sub_df_fillna = flagged_df\n",
    "        sub_df_fillna.loc[sub_df_fillna[f].isin([-np.inf, np.inf, np.nan]), f] = sub_df_fillna.loc[~sub_df_fillna[f].isin([-np.inf, np.inf, np.nan]), f].median()\n",
    "\n",
    "        aucs_dropna = []\n",
    "        aucs_fillna = []\n",
    "        for t in targets:\n",
    "            fpr, tpr, _ = roc_curve(sub_df_dropna[t], sub_df_dropna[f], pos_label=1)\n",
    "            aucs_dropna.append(auc(fpr, tpr))\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(sub_df_fillna[t], sub_df_fillna[f], pos_label=1)\n",
    "            aucs_fillna.append(auc(fpr, tpr))\n",
    "        \n",
    "        app_df = pd.DataFrame(columns=['feature', 'flag', 'missing'] + ['auc_' + c + '_dropna' for c in targets]+ ['auc_' + c + '_fillna' for c in targets], \n",
    "            data=np.array([f, flag, missing] + aucs_dropna + aucs_fillna).reshape(1, -1))\n",
    "        stat_df = pd.concat([stat_df, app_df], axis=0)\n",
    "\n",
    "    for t in targets:\n",
    "        stat_df['auc_' + t  + '_dropna'] = stat_df['auc_' + t  + '_dropna'].astype(float)\n",
    "        stat_df['absolute_auc_' + t  + '_dropna'] = 0.5 + (stat_df['auc_' + t + '_dropna'] - 0.5).abs()\n",
    "\n",
    "        stat_df['auc_' + t  + '_fillna'] = stat_df['auc_' + t  + '_fillna'].astype(float)\n",
    "        stat_df['absolute_auc_' + t  + '_fillna'] = 0.5 + (stat_df['auc_' + t + '_fillna'] - 0.5).abs()\n",
    "\n",
    "    return stat_df.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
